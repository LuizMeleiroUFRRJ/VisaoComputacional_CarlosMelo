# -*- coding: utf-8 -*-
"""Aula_004_Redes_Neurais.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yTwYp4NIBDgV_6s_eeaoTpaBQLZxBcxU

# Redes Neurais

Redes neurais (NN) são os blocos construtores fundamentais do Deep Learning. Sua popularidade se dá devido a capacidade de lidar com problemas altamente complexos, onde algoritmos tradicionais não foram capazes de obter sucesso.

Entre as principais aplicações das NN estão os carros autônomos, reconhecimento de objetos em imagens, tradução entre idiomas, legendas automáticas em vídeos, entre outras.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/human-neuron.png" height="400px"></p>

Como foi mencionado na apresentação de slides, as NN são inspiradas neurônios biológicos da vida real, aqueles que temos no nosso próprio sistema nervoso - aproximadamente 10 bilhões.

Cada um dos nossos neurônios está conectado a cerca de 10 mil outros neurônios. A comunicação entre esses neurônios ocorre por meio de impulsos captados pelos dendritos. Na sequência, esses impulsos são transmitidos pelo corpo do neurônio, por meio do axônio, até atingirem os dendritos de neurônios vizinhos através de sinapses.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/simple-nn.png" height="300px"></p>

Se ficou difícil de imaginar esse paralelo entre o mundo real e artificial, veja a imagem abaixa comparando redes neurais biológicas e artificiais, bio-inspirados:

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/comparativo_nn.png" height="250px"></p>

Abandonando um pouco

Em um NN artificial, recebem-se valores $x_1, x_2, x_3$, jutamente com uma constante conhecida como *bias*, que são multiplicados por pesos $w_1, w_2, w_3, w_4$ e somados. Por fim, essa soma passa por uma função de ativação, que irá fornecer o *output*.

Matematicamente, a saída (*output*) da NN pode ser escrita de várias formas diferentes. Tipicamente você irá encontrar  a forma matemática, escrita como uma única equação:

$$
\hat{y} = g \left(w_0 + \sum_{i=1}^{m} x_iw_i \right) \\
$$

ou podemos escrever a mesma coisa usando a Algebra Linear em termos de vetores e produtos escalares:

$$
\hat{y} = g \left( w_0 + X^T W \right) \\
$$

$$
\begin{equation}
\begin{aligned}
X&=
    \begin{bmatrix}
        x_1 \\ \vdots \\ x_2
    \end{bmatrix}
&&W=
    \begin{bmatrix}
        w_1 \\ \vdots \\ w_m
    \end{bmatrix} \\
\end{aligned}
\end{equation}
$$

### Um exemplo de Rede Neural simples

Veja a imagem abaixo, onde temos uma NN que recebe dois *inputs* e fornece um resultado de saída.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/exemplo-nn.png" height="250px"></p>

Nessa situação acima temos:

$$
\begin{equation}
\begin{aligned}
X&=
    \begin{bmatrix}
        x_1 \\ x_2
    \end{bmatrix}
&&W=
    \begin{bmatrix}
        -2 \\ \\ 5
    \end{bmatrix}
&&w_0=1
\end{aligned}
\end{equation}
$$

Matematicamente, o resultado final $\hat{y}$ poderia ser escrito como $\hat{y} = g(1 - 2x_1 +5x_2)$. No entanto, isso resolve apenas uma parte do problema.

O poder do *Deep Learning* está em usar a não-lineariedade para resolver problemas complexos. É aí que entra a **função de ativação**, tema muito amplo (e que terá uma aula exclusiva para ele).

Vamos apenas assumir que será usada a **função sigmoidal** (e sua curva ***sigmoidal***) para fornecer à nossa NN essa tal não-lineariedade.

$$
g(z) = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

Não entendeu essa questão da lineariedade?

Visualmente, veja na figura abaixo (extraída do MIT) como a parte linear $w_0 + \sum_{i=1}^{m} x_iw_i$ da nossa equação não conseguiria separar corretamente as classes, não importando quantas camadas de neurônios existissem.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/linear-vs-nao-linear.png" height="250px"></p>

Já quando você utiliza essa parte linear dentro de uma função de ativação $\sigma(w_0 + \sum_{i=1}^{m} x_iw_i)$, possibilita que esse tipo de curva não-linear possa ocorrer.
"""

# Commented out IPython magic to ensure Python compatibility.
# importar as bibliotecas necessárias
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# configurações do notebook
sns.set_style()

# importar dataset simplificado (variáveis numéricas)
from sklearn.datasets import fetch_california_housing

# exemplo dado na aula de rede neural:
def sigmoid(x):
    return 1.0/(1 + np.exp(-x))

# valores de input
x1 = 10
x2 = -2

# soma de w0 + X.W
res = 1 -2*x1 + 5*x2

# função de ativação sigmoid
yhat = sigmoid(res)
print(yhat)

"""## Redes Neurais ao *dataset* imobiliário da California"""

# importar o dataset e lista com nomes das features
dataset = fetch_california_housing()
features = dataset.feature_names

# dividir entre treino, validação e teste
X_train_original, X_test, y_train_original, y_test = train_test_split(dataset.data, dataset.target)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_original, y_train_original)

# ver como ficaria em formato de DataFrame
df = pd.DataFrame(X_train)
df.columns = features
df.head()

# padronizar os dados com StandardScaler por causa do Gradient Descent
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

# ver o dataset padronizado
pd.DataFrame(X_train).head()

# construir uma nn
model = keras.models.Sequential(
    [
     keras.layers.Dense(30, activation='relu', input_shape=(X_train.shape[1:])),
     keras.layers.Dense(1)
    ]
)

# compilar a nn
model.compile(loss='mean_squared_error', optimizer='sgd')

# obter o histórico de loss
history = model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid))

# verificar o MSE
error = model.evaluate(X_test, y_test)

# fazer uma nova previsão
new_house = X_train[0].reshape(1,-1)
model.predict(new_house)

# plotar historico
pd.DataFrame(history.history).plot();

