# -*- coding: utf-8 -*-
"""Aula_008_Classificação_de_Roupas_usando_Deep_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-CtKcFUGGWhgIIgTpwD6-UDm8DeD2Eh5

# Classificação de Roupas usando Deep Learning

***Fashion MNIST*** é um *dataset* padrão usado em **Deep Learning** e **Computer Vision**, contendo 70 mil imagens de 10 classes diferentes.

Apesar de trazer imagens já pré-processadas, representa um bom desafio para aplicar redes neurais profundas. Especificamente, serve para demonstrar um tipo especial das redes neurais, as redes neurais convolucionais (*Convolutional Neural Networks* - **CNN**).

O *dataset* é dividido entre treino (60 mil amostras) e teste (10 mil amostras), sendo que cada imagem possui dimensões de 28 x 28 *pixels*.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/fashion-mnist-sprite.png" height="400px"></p>

Cada imagem pertence exclusivamente a uma única classe. A tabela abaixo segue a documentação do Fashion MNIST, onde são documentados os 10 *labels* possíveis:

<table>
  <tbody><tr>
    <th>Label</th>
    <th>Class</th>
  </tr>
  <tr>
    <td>0</td>
    <td>T-shirt/top</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Trouser</td>
  </tr>
    <tr>
    <td>2</td>
    <td>Pullover</td>
  </tr>
    <tr>
    <td>3</td>
    <td>Dress</td>
  </tr>
    <tr>
    <td>4</td>
    <td>Coat</td>
  </tr>
    <tr>
    <td>5</td>
    <td>Sandal</td>
  </tr>
    <tr>
    <td>6</td>
    <td>Shirt</td>
  </tr>
    <tr>
    <td>7</td>
    <td>Sneaker</td>
  </tr>
    <tr>
    <td>8</td>
    <td>Bag</td>
  </tr>
    <tr>
    <td>9</td>
    <td>Ankle boot</td>
  </tr>
</tbody></table>

Este *dataset* foi criado em substituição ao MNIST tradicional, onde as imagens eram puramente de dígitos manuscritos. Dentre os motivos principais, o MNIST origianl era:

* Muito fácil, uma vez que as CNN conseguiam atingir tranquilamente a acurácia de 99,7%.
* Não representava mais os desafios modernos da área da Visão Computacional.

<p align="center"><img src="https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/embedding.gif" height="300px"></p>

## Obtenção dos dados

Existem duas maneiras de se obter os dados do Fashion MNIST:

* [Diretamente a partir do repositório no Github](https://github.com/zalandoresearch/fashion-mnist)
* Carregando a partir do TensorFlow.

Para este projeto, irei carregar os dados a partir do próprio TensorFlow.
"""

# Commented out IPython magic to ensure Python compatibility.
# importar as bibliotecas necessárias
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow import keras
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report

# configurar a visualização
# %matplotlib inline
# %config InlineBackend.figure_format = 'svg'
mpl.style.use( 'ggplot' )
plt.style.use('fivethirtyeight')
sns.set(context="notebook", palette="dark", style = 'whitegrid' , color_codes=True)

# carregar os dados do Fashion MNIST
(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = keras.datasets.fashion_mnist.load_data()

"""Seguindo a padronização da documentação, criarei uma lista contendo os *labels* traduzidos para o portugês."""

# De acordo com a documentação, os labels são:
class_names = ['Camisetas/Top', 'Calça', 'Suéter',
               'Vestidos','Casaco', 'Sandálias',
               'Camisas', 'Tênis', 'Bolsa', 'Botas']

"""## Exploração dos Dados

Apesar desse conjunto de dados vir previamente processado e bem documentado, exploraremos rapidamente a fim de tirar alguns *insights*. Primeiramente, irei verificar as dimensões dos *arrays* retornados pela função `fashion_mnist.load_data()`.

Conforme visto anteriormente, há 60.000 amostras de treino e 10.000 de teste, sendo as imagens 28 x 28 *pixels* em tons de cinza (1 canal).

Também é possível ver como uma imagem "se parece" quando é convertida para um *array*. Vale lembrar que os valores de cada *pixel* estão compreendidos entre $[0, 255]$.
"""

# ver a dimensionalidade dos DataFrames
print("Dimensionalidade dos DataFrames:")
print("X_train_orig:", X_train_orig.shape)
print("y_train_orig:", y_train_orig.shape)
print("X_test_orig:", X_test_orig.shape)
print("y_test_orig:", y_test_orig.shape)

# ver uma fatia de uma imagem
print("\n\nImagem convertida em array:\n", X_train_orig[0][:15][:15])

"""Irei verificar se os datasets de treino/teste estão balanceados adequadamente. Ou seja, se contém uma proporção ideal entre as diferentes classes.


"""

# verificar os valores únicos por classes (treino)
print("y_train_orig:")
np.unique(y_train_orig, return_counts=True)

# verificar os valores únicos por classes (teste)
print("y_test_orig:")
np.unique(y_test_orig, return_counts=True)

"""Há pouco a se explorar nesta análise exploratória preliminar, tratando-se mais de uma etapa visando aumentar a consciência situacional a respeito do *dataset*.

Por fim, vamos visualizar algumas imagens com seus *labels*.
"""

# ver algumas imagens de exemplo
plt.figure(figsize=(6,6))

for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train_orig[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[y_train_orig[i]])
plt.tight_layout()

"""## Pré-Processamento dos Dados

Antes de criar o modelo e treinar a rede neural, irei passar por algumas etapas de pré-processamento.

### Normalizar os *pixels*

As intensidades dos *pixels* devem ser normalizadas, Ou seja, os valores inteiros devem ser convertidos no tipo `float` e ficar dentro do intervalo $[0, 1]$.

Aqui, como o valor máximo do *pixel* é 255, basta fazer a divisão de todos *pixels* individuais por 255.0 e eles estarão normalizados e do tipo `float`.
"""

# criar função lambda que transforma em float32 e normaliza os pixels
f = lambda x: (x / 255.0).astype("float32")

# aplicar a função lambda aos datasets X_train e X_test
X_train = f(X_train_orig)
X_test = f(X_test_orig)

"""### Redimensionar as Imagens

O primeiro *layer* convolucional espera um único tensor que contenha todos os *pixels*. O TensorFlow espera uma única lista com 4 dimensões, e não 60000 itens de dimensões 28 x 28 x 1, como é o caso do *dataset* de treino (o mesmo vale para os dados de teste).
"""

# redimensionar as imagens
X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))
X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))

print("X_train:{}".format(X_train.shape))
print("X_test:\t{}".format(X_test.shape))

"""### One-Hot Encoding

Os *labels* já estão codificados entre 0 e 9, naquilo que chamamos de *Integer Encoding*. É possível utilizar diretamente `y_train` e `y_test`, desde que se use `loss="sparse_categorical_crossentropy"`.

No entanto, utilizá-los diretamente pode levar o modelo de *Deep Learning* a entenderque há uma relação natural de ordem entre os *labels*, o que não é verdade - podendo levar a um desempenho inferior.

Para variáveis categóricas onde não há nenhuma relação ordinal, prefiro usar o método One-Hot Encoding. Para saber mais sobre as diferenças entre essas duas técnicas, [leia este artigo](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).

Veja um exemplo prático como funciona esse tipo de codificação.


"""

exemplo = np.array([1, 3, 4, 2, 0])
print("Exemplo antes do Encoding:")
print(exemplo)

exemplo_encoded = keras.utils.to_categorical(exemplo)
print("\nExemplo depois do Encoding")
print(exemplo_encoded)

"""Da mesma maneira que o exemplo acima, vamos aplicar o *one-hot encoding* aos *labels*."""

y_train = keras.utils.to_categorical(y_train_orig)
y_test = keras.utils.to_categorical(y_test_orig)

"""## Definindo uma Rede Neural Convolucional

Existem diversas arquiteturas consagradas de CNN. Para este projeto, será utilizada uma arquitetura simplificada da VGGNet, implementada em um artigo do Cientista de Dados Adrian Rosebroke.

A implementação original foi adaptada por mim, a fim de adequar as dimensões 28 x 28 das nossas imagens e permitir o uso da API Keras.
"""

# baseado na implementação da MiniVGGNet do Adrian Rosebroke
# first CONV => RELU => CONV => RELU => POOL layer set
model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, 3, padding="same", activation='relu',))
model.add(keras.layers.BatchNormalization(axis=1))
model.add(keras.layers.Conv2D(32, (3, 3), padding="same", activation='relu'))
model.add(keras.layers.BatchNormalization(axis=1))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Dropout(0.25))

# second CONV => RELU => CONV => RELU => POOL layer set
model.add(keras.layers.Conv2D(64, (3, 3), padding="same", activation='relu'))
model.add(keras.layers.BatchNormalization(axis=1))
model.add(keras.layers.Conv2D(64, (3, 3), padding="same", activation='relu'))
model.add(keras.layers.BatchNormalization(axis=1))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Dropout(0.25))

# first (and only) set of FC => RELU layers
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(512, activation='relu'))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dropout(0.5))

# softmax classifier
model.add(keras.layers.Dense(10, activation='softmax'))

"""Como mencionado anteriormente, uma vez que os *labels* estão *one-hot encoded*, será utilizado `loss="categorical_crossentropy"` para compilar o modelo.

Também será informado explicitamente o uso de dados de validação na proporção de 1/3.
"""

# model.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=['accuracy'])
model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])

# treinar o modelo e salvar as informações em history
history = model.fit(X_train, y_train, epochs=20, validation_split=0.3)

"""## Avaliando o Modelo

No geral, o nosso modelo atingiu uma acurácia de XX nos dados de treino e XX nos dados de validação.

No entanto, se observarmos melhor veremos que ela não teve o desempenho bom para a categoria "Camisas". Provavalmente isso pode ser melhorado com técnicas de *data augmentation*.
"""

y_hat = model.predict(X_test)
y_hat_classes = np.argmax(y_hat, axis=1)
print(classification_report(y_test_orig, y_hat_classes, target_names=class_names))

"""Olhando as curvas *accuracy* e *val_accuracy*, identifica-se talvez um pequeno *overfitting*, mas que não prejudicou muito o desempenho da CNN perante aos dados de validação."""

# Assumindo que já treinamos o modelo e temos os pesos salvos
# model.load_weights('caminho_para_os_pesos_salvos')

# Vamos inspecionar a primeira camada convolucional
first_conv_layer = model.layers[0]

# Extrair os pesos da primeira camada convolucional
first_conv_weights = first_conv_layer.get_weights()[0]
first_conv_biases = first_conv_layer.get_weights()[1]

print("Pesos da primeira camada convolucional:")
print(first_conv_weights)

print("Biases da primeira camada convolucional:")
print(first_conv_biases)

# Para visualizar os pesos, podemos usar matplotlib
import matplotlib.pyplot as plt

# Vamos plotar os primeiros 6 filtros
num_filters_to_show = 6

fig, axs = plt.subplots(1, num_filters_to_show, figsize=(20, 20))

for i in range(num_filters_to_show):
    axs[i].imshow(first_conv_weights[:, :, 0, i], cmap='gray')
    axs[i].axis('off')

plt.show()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K

# Função para visualizar as ativações das camadas
def get_layer_outputs(model, image):
    # Criar um modelo que retorna as saídas de cada camada
    layer_outputs = [layer.output for layer in model.layers]
    activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)

    # Obter as ativações para a imagem fornecida
    activations = activation_model.predict(image)
    return activations

# Função para plotar as ativações
def plot_layer_activations(activations, layer_names, image_number=0, col_size=6, row_size=6):
    for layer_activation, layer_name in zip(activations, layer_names):
        num_filters = layer_activation.shape[-1]
        size = layer_activation.shape[1]

        # Ajuste o tamanho da grade de acordo com o número de filtros
        n_cols = col_size
        n_rows = num_filters // n_cols

        display_grid = np.zeros((size * n_rows, n_cols * size))

        for col in range(n_cols):
            for row in range(n_rows):
                filter_index = col + (row * n_cols)
                if filter_index < num_filters:
                    channel_image = layer_activation[image_number, :, :, filter_index]
                    channel_image -= channel_image.mean()
                    channel_image /= (channel_image.std() + 1e-5)
                    channel_image *= 64
                    channel_image += 128
                    channel_image = np.clip(channel_image, 0, 255).astype('uint8')
                    display_grid[row * size : (row + 1) * size,
                                 col * size : (col + 1) * size] = channel_image

        scale = 1. / size
        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))
        plt.title(layer_name)
        plt.grid(False)
        plt.imshow(display_grid, aspect='auto', cmap='viridis')

# Supondo que temos uma imagem de teste no formato esperado pelo modelo
image = X_test[0:1]

# Obter nomes das camadas para facilitar a visualização
layer_names = [layer.name for layer in model.layers if 'conv' in layer.name or 'pool' in layer.name]

# Obter as ativações das camadas
activations = get_layer_outputs(model, image)

# Plotar as ativações das camadas
plot_layer_activations(activations, layer_names)

# plotar o histórico da otimização
pd.DataFrame(history.history).plot()
plt.show()

"""Por fim, vemos que a acurácia no *dataset* de teste se mantém muito boa, o que significa que o modelo é genérico o suficiente para lidar com dados novos."""

score = model.evaluate(X_test, y_test)

# verificar o desempenho do modelo
print('Loss: {:.4f}'.format(score[0]))
print('Acurácia: {:.4f}'.format(score[1]))

"""## Melhorias futuras..."""

